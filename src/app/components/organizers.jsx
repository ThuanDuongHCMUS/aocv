const Organizers = ()=>{
    return <div className="text-justify"> 
         <span className="font-bold">The organizing committee have experience in this area and rigorously published papers in this research direction:  </span>
         <ul class="list-disc list-inside space-y-2 mt-2">
         <li>JiaXuan Li, Duc Minh Vo, Akihiro Sugimoto, Hideki Nakayama, &quot;EVCap: Retrieval-Augmented Image Captioning with External Visual-Name Memory for Open-World Comprehension,&quot; IEEE Computer Vision and Pattern Recognition (CVPR), 2024 (to appear)</li>
        <li>Duc Minh Vo, Quoc-An Luong, Akihiro Sugimoto, Hideki Nakayama, &quot;A-CAP: Anticipation Captioning with Commonsense Knowledge,&quot; IEEE Computer Vision and Pattern Recognition (CVPR), 2023</li>
        <li>Duc Minh Vo, Hong Chen, Akihiro Sugimoto, Hideki Nakayama, &quot;NOC-REK: Novel Object Captioning with Retrieved Vocabulary from External Knowledge,&quot; IEEE Computer Vision and Pattern Recognition (CVPR), 2022</li>
        <li>Duc Minh Vo, Akihiro Sugimoto, &quot;Visual-Relation Conscious Image Generation from Structured-Text,&quot; European Conference on Computer Vision (ECCV), 2020</li>
        <li>Dinh-Khoi Vo, Duy-Nam Ly, Khanh-Duy Le, Tam V. Nguyen, Minh-Triet Tran, Trung-Nghia Le, &quot;iCONTRA: Toward Thematic Collection Design Via Interactive Concept Transfer,&quot; CHI, 2024</li>
        <li>Minh-Hien Le, Chi-Bien Chu, Khanh-Duy Le, Tam V. Nguyen, Minh-Triet Tran, Trung-Nghia Le, &quot;VIDES: Virtual Interior Design via Natural Language and Visual Guidance,&quot; ISMAR, 2023</li>
        <li>Fatma Shalabi, Hichem Felouat, Huy H. Nguyen, and Isao Echizen, &quot;Leveraging Chat-Based Large Vision-Language Models for Multimodal Out-of-Context Detection,&quot; International Conference on Advanced Information Networking and Applications (AINA), 2024.</li>
        <li>Fatma Shalabi*, Huy H. Nguyen*, Hichem Felouat, Ching-Chun Chang, and Isao Echizen, &quot;Image-Text Out-Of-Context Detection Using Synthetic Multimodal Misinformation,&quot; Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC) 2023.</li>
        </ul>
         <span className="font-bold">The names and full contact information (email and postal addresses, fax and telephone numbers) of the organizing committee. </span>
         <ul class="list-disc list-inside space-y-4">
            <li><span className="font-bold">Minh-Duc Vo</span>, The University of Tokyo, 7-3-1 Hongo, Bunkyo city, Tokyo, Japan 113-8654. Email: vmduc@nlab.ci.i.u-tokyo.ac.jp (main contact). <br/> Minh-Duc Vo received the Ph.D. degree in computer science from The Graduate University for Advanced Studies (SOKENDAI) in alliance with National Institute of Informatics (NII), Japan, in 2020. He is currently a Project Assistant Professor at The University of Tokyo. His research interests include image recognition and generation and language and vision. He is a regular reviewer of international conferences in computer vision.</li>
            <li><span className="font-bold">Akihiro Sugimoto</span>, National Institute of Informatics, 2-1-2 Hitotsubashi, Chiyoda City, Tokyo, Japan, 101-8430. Email: sugimoto@nii.ac.jp.<br/> Akihiro Sugimoto received the Dr. Eng. in mathematical engineering from the University of Tokyo. He is currently a full professor at the National Institute of Informatics, Tokyo. He has regularly served as an Area Chair at several top-tier conferences including ICCV, CVPR, ECCV, ICLR, NeurIPS, ACCV, ICPR, and 3DV. He also served an associate editor of IJCV, and is now serving as an associate editor of CVIU. He served as GC of ACCV2012, 2020, and 3DV2020. He has published more than 150 peer-reviewed journal/international conference papers.</li>
            <li><span className="font-bold">Hideki Nakayama</span>, The University of Tokyo, 7-3-1 Hongo, Bunkyo city, Tokyo, Japan 113-8654. Email: nakayama@ci.i.u-tokyo.ac.jp. <br/>Hideki Nakayama received the Ph.D. degree in information science from The University of Tokyo, Japan, in 2011. From 2012 to 2018, he was an Assistant Professor with the Graduate School of Information Science and Technology, The University of Tokyo, where he has been an Associate Professor since April 2018. He is also a Faculty Member with the International Research Center for Neurointelligence (IRCN) and the Institute of AI and Beyond (BeyondAI), The University of Tokyo. His research interests include machine perception, natural language understanding, contents generation, and multimodal learning. He has published 100+ peer-reviewed papers including 50+ renowned conferences and journals such as CVPR, ICCV, ECCV, ACL, NAACL, EMNLP, TACL, ICLR, AAAI. Also, he has served as Senior Area Chair or Area Chair for many top conferences including CVPR, ICCV, NAACL, NeurIPS, ICLR, and IJCAI. He is a member of IEEE and ACM.</li>
            <li><span className="font-bold">Minh-Triet Tran</span>, University of Science, VNU- HCM, 227 Nguyen Van Cu, Ward 4, District 5, Ho Chi Minh City, Vietnam, 70000. Email: tmtriet@fit.hcmus.edu.vn. <br/>Minh-Triet Tran is currently the Vice President of University of Science, VNU-HCM, Vietnam. He is also the Head of Software Engineering Laboratory and the Deputy Head of Artificial Intelligence Laboratory, University of Science. He obtained his B.Sc., M.Sc., and Ph.D. degrees in computer science from University of Science, VNU-HCM, in 2001, 2005, and 2009. His research interests include cryptography and security, computer vision and machine learning, and human-computer interaction. He was a visiting scholar at National Institutes of Informatics (NII, Japan) in 2008, 2009, and 2010, and at University of Illinois at Urbana-Champaign (UIUC, US) in 2015-2016.</li>
            <li><span className="font-bold">Huy H. Nguyen</span>, National Institute of Informatics, 2-1-2 Hitotsubashi, Chiyoda City, Tokyo, Japan, 101-8430. Email: nhhuy@nii.ac.jp.<br/> Hong-Huy Nguyen received a Ph.D. degree in computer science from the Graduate University for Advanced Studies, SOKENDAI, in alliance with the National Institute of Informatics, Japan, in 2022. He is currently a project assistant professor in the Echizen Laboratory at the National Institute of Informatics, Tokyo, Japan. His research interests include biometrics, security and privacy for machine learning, and disinformation.</li>
            <li><span className="font-bold">Trung-Nghia Le</span>, University of Science, VNU- HCM, 227 Nguyen Van Cu, Ward 4, District 5, Ho Chi Minh City, Vietnam, 70000. Email: ltnghia@fit.hcmus.edu.vn.<br/> Trung-Nghia Le is currently a senior researcher and lecturer at Faculty of Information Technology, University of Science, VNU-HCM, Vietnam. Prior to that, he was a project assistant professor at National Institute of Informatics, Japan. He received his Ph.D degree from National Institute of Informatics in 2018. His research topics include computer vision, applied deep learning, multimedia systems, multimedia security, extended reality, and human-computer interaction.</li>
            <li ><span className="font-bold">Khan Md Anwarus Salam</span>, SoftBank, 1-7-1 Kaigan, Minato-ku, Tokyo, Japan. 105-7529. Email: khan.mdanwarussalam@g.softbank.co.jp.<br/> Dr. Khan Md. Anwarus Salam is working in SoftBank’s Beyond AI Promotion division. His extensive career includes roles as a Research Scientist at IBM Research in Tokyo and as the country engineering consultant for Google in Bangladesh. Academically, he holds a Ph.D. and Master’s in Information and Communication Engineering from The University of Electro-Communications in Tokyo, where he researched machine translation, and a B.Sc. in Computer Science from BRAC University, Dhaka. His research interests include Generative AI, Natural Language Processing, Semantic Analysis, Machine Translation and Machine Learning. Dr. Salam’s work significantly influences both academic research and practical applications in AI, shaping the future of technology.</li>
            <li><span className="font-bold">TASUKI Team</span>, SoftBank, 1-7-1 Kaigan, Minato-ku, Tokyo, Japan. 105-7529.</li>
        </ul>

        <p className="mt-5">
        The primary goal of this workshop challenge is to advance the capability of Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) to accurately interpret and generate descriptive text from complex visual inputs such as charts, diagrams, and building design drawings. By leveraging the benchmark dataset provided by the TASUKI team from Softbank, participants will work towards improving the models&apos; understanding and transcription of graphical data into markdown format. 
        </p>
        <p className="mt-2">
        Challenge Overview: Participants will develop and fine-tune LLMs and LVLMs to read and transcribe a variety of diagrams and charts, ensuring accurate interpretation and description. The workshop will focus on different types of visual data such as Data Flow Diagrams (DFDs), Class Diagrams, Gantt Charts, Building Design Drawings, etc. The models should produce transcriptions in mermaid markdown format, capturing all essential details and nuances present in the visual inputs. 
        </p>
        <p className="mt-2">
            Dataset: The TASUKI team&apos;s benchmark dataset includes paired data consisting of visual representations (DFDs, class diagrams, Gantt charts, building design drawings, etc.) and their corresponding markdown transcriptions. This dataset serves as the foundation for training and evaluating the models developed during the workshop.  
        </p>
        <p className="mt-2 mb-5">
        Computational Resources: Access to Beyond AI SANDBOX GPUs for University of Tokyo participants. 
        </p>

        <span className="font-bold mt-5">Key Tasks:  </span>
        <ol class="list-decimal list-inside space-y-4 mb-5">
            <li>
                <strong>Data Preprocessing and Augmentation:</strong>
                <ul class="list-disc list-inside pl-6 space-y-2">
                    <li>Ensure the dataset is in a format suitable for training models.</li>
                    <li>Augment the dataset to enhance model robustness.</li>
                </ul>
            </li>
            <li>
                <strong>Model Development and Fine-Tuning:</strong>
                <ul class="list-disc list-inside pl-6 space-y-2">
                    <li>Fine-tune existing LLMs and LVLMs on the provided dataset.</li>
                    <li>Experiment with different architectures and training strategies to optimize performance.</li>
                </ul>
            </li>
            <li>
                <strong>Evaluation and Benchmarking:</strong>
                <ul class="list-disc list-inside pl-6 space-y-2">
                    <li>Evaluate model performance using standard metrics for text generation and transcription accuracy.</li>
                    <li>Compare results against baseline models and previous benchmarks.</li>
                </ul>
            </li>
            <li>
                <strong>Error Analysis and Iterative Improvement:</strong>
                <ul class="list-disc list-inside pl-6 space-y-2">
                    <li>Conduct detailed error analysis to identify common failure modes.</li>
                    <li>Iteratively improve model performance based on insights gained from error analysis.</li>
                </ul>
            </li>
        </ol>
        <span className="font-bold mt-5">Expected Outcomes:  </span>
        <ol class="list-decimal list-inside space-y-4 mb-5">
            <li>
                <strong>Improved Models:</strong> 
                Enhanced LLMs and LVLMs that can accurately read and transcribe complex diagrams and charts into markdown format.
            </li>
            <li>
                <strong>Benchmarking Results:</strong> 
                Comprehensive benchmarking results detailing the performance of various models on the provided dataset.
            </li>
            <li>
                <strong>Documentation:</strong> 
                Detailed documentation of methodologies, challenges, and solutions encountered during the workshop.
            </li>
            <li>
                Insights and recommendations for future research and development in the field of Graph2Text.
            </li>
        </ol>
        <span className="font-bold mt-5">Participation:  </span>
        <p>
        This workshop challenge aims to push the boundaries of what is possible with LLMs and LVLMs in interpreting and transcribing graphical data. By leveraging the TASUKI benchmark dataset, participants will contribute to significant advancements in Graph2Text technologies, with potential applications in various domains requiring accurate and detailed visual data interpretation. We invite AI researchers, data scientists, and practitioners with interest and experience in natural language processing, computer vision, and multimodal learning to join this workshop challenge. Participants can register as individuals or teams. 
        </p>
    </div>
}
export default Organizers;